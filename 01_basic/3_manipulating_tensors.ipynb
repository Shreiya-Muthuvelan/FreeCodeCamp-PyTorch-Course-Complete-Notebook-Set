{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05eec506",
   "metadata": {},
   "source": [
    "## Reshaping, Stacking, Squeezing and Unsqueezing tensors\n",
    "\n",
    "* Reshaping- reshapes an input tensor to a defined shape\n",
    "* View- return a view of an input tensor of a certain shape but keep the same memory as the original tensor\n",
    "* Stacking- combine multiple tensors on top of each other (vstack) or side by side (hstack)\n",
    "* Squeeze- removes all `1` dimensions from a tensor\n",
    "* Unsqueeze- add a `1` dimension to a target tensor\n",
    "* Permute - return a view of the input with dimensions permuted(swapped) in a certain way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d229255b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]), torch.Size([9]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.arange(1.,10.)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdac4cd",
   "metadata": {},
   "source": [
    "### **Reshape Operation**\n",
    "\n",
    "***torch.reshape(input, shape) -> Tensor***\n",
    "\n",
    "args:\n",
    "\n",
    "- input(Tensor): the tensor to be reshaped\n",
    "\n",
    "- shape(tuple of int): the new shape\n",
    "\n",
    "Returns a tensor with same data and number of elements as input , but with the specified shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2468eef",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 7]' is invalid for input of size 9",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Add an extra dimension\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# The reshape operation should be compatible with the number of elements in the tensor \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m x_reshaped\u001b[38;5;241m=\u001b[39m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m x_reshaped,x_reshaped\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 7]' is invalid for input of size 9"
     ]
    }
   ],
   "source": [
    "# Add an extra dimension\n",
    "# The reshape operation should be compatible with the number of elements in the tensor \n",
    "x_reshaped=x.reshape(1,7)\n",
    "x_reshaped,x_reshaped.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fac924",
   "metadata": {},
   "source": [
    "We get an error here as we are trying to squeeze 9 elements into 7 which is not possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd5d1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]]), torch.Size([1, 9]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped=x.reshape(1,9)\n",
    "x_reshaped,x_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc82fd97",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 9]' is invalid for input of size 9",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_reshaped\u001b[38;5;241m=\u001b[39m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m x_reshaped,x_reshaped\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[2, 9]' is invalid for input of size 9"
     ]
    }
   ],
   "source": [
    "x_reshaped=x.reshape(2,9)\n",
    "x_reshaped,x_reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b957ffc",
   "metadata": {},
   "source": [
    "This wil also not work as we are trying to reshape 9 elements into 18 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fcf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [2.],\n",
       "         [3.],\n",
       "         [4.],\n",
       "         [5.],\n",
       "         [6.],\n",
       "         [7.],\n",
       "         [8.],\n",
       "         [9.]]),\n",
       " torch.Size([9, 1]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped_n=x.reshape(9,1)\n",
    "x_reshaped_n,x_reshaped_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c6762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]), torch.Size([10]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.arange(1.,11.)\n",
    "y, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937add5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.,  9., 10.]]),\n",
       " torch.Size([2, 5]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_reshaped=y.reshape(2,5)\n",
    "y_reshaped,y_reshaped.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff16532",
   "metadata": {},
   "source": [
    "This works because 2x5=10 and the inital shape was also 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd470c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.]),\n",
       " torch.Size([12]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.arange(1.,13.)\n",
    "y, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d1f703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  2.,  3.,  4.],\n",
       "         [ 5.,  6.,  7.,  8.],\n",
       "         [ 9., 10., 11., 12.]]),\n",
       " torch.Size([3, 4]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_reshaped=y.reshape(3,4)\n",
    "y_reshaped,y_reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808cb34f",
   "metadata": {},
   "source": [
    "### **View Operation** \n",
    "\n",
    "***torch.view(***shape****) *-> Tensor***\n",
    "\n",
    "args:\n",
    "\n",
    "- shape(torch.Size or int): the desired size\n",
    "\n",
    "Returns a new tensor with same data as self tensor but a different shape\n",
    "\n",
    "**NOTE:** Very similar to Reshape operation, only difference is the view shares the same memory as the original tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1d9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]]), torch.Size([1, 9]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the view\n",
    "\n",
    "z=x.view(1,9)\n",
    "z,z.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a570576b",
   "metadata": {},
   "source": [
    "**NOTE:** Changing z changes x (because a view of tensor shares the same memory of the original tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa329a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]]),\n",
       " tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:,0]=5\n",
    "z,x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd572e",
   "metadata": {},
   "source": [
    "### **Stack  Operation**\n",
    "\n",
    "***torch.stack(tensors, dim=0,out=None) -> Tensor***\n",
    "\n",
    "args:\n",
    "\n",
    "- tensors: sequence of tensors to concaenate\n",
    "\n",
    "- dim(int,optional): dimension to insert. Default is 0 which inserts along rows \n",
    "    \n",
    "- out(Tensor,optional): the output tensor\n",
    "\n",
    "Concatenates a sequence of tensors along a new dimension. All tensors need to be of same size\n",
    "\n",
    "***torch.hstack(tensors,out=None) -> Tensor***\n",
    " \n",
    "- Stack the tensors in sequence horizontally (column wise)\n",
    "\n",
    "***torch.vstack(tensors,out=None) -> Tensor***\n",
    "\n",
    "- Stack the tensors in sequence vertically (row wise). Equivalent to stack with dim=0 for 1D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc09631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [5., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [5., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [5., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [5., 2., 3., 4., 5., 6., 7., 8., 9.]]),\n",
       " torch.Size([5, 9]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack tensors on top of each other\n",
    "x_stacked=torch.stack([x,x,x,x,x],dim=0)\n",
    "x_stacked,x_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba71eae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 5., 5., 5., 5.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [4., 4., 4., 4., 4.],\n",
       "         [5., 5., 5., 5., 5.],\n",
       "         [6., 6., 6., 6., 6.],\n",
       "         [7., 7., 7., 7., 7.],\n",
       "         [8., 8., 8., 8., 8.],\n",
       "         [9., 9., 9., 9., 9.]]),\n",
       " torch.Size([9, 5]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_stacked=torch.stack([x,x,x,x,x],dim=1)\n",
    "x_stacked,x_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f4a8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5., 2., 3., 4., 5., 6., 7., 8., 9., 5., 2., 3., 4., 5., 6., 7., 8., 9.,\n",
       "         5., 2., 3., 4., 5., 6., 7., 8., 9., 5., 2., 3., 4., 5., 6., 7., 8., 9.,\n",
       "         5., 2., 3., 4., 5., 6., 7., 8., 9.]),\n",
       " torch.Size([45]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_h_stacked=torch.hstack([x,x,x,x,x])\n",
    "x_h_stacked,x_h_stacked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67e9fca",
   "metadata": {},
   "source": [
    "hstack is the equivalent of concatenation along the first axis for 1-D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd93c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [5., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [5., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [5., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "         [5., 2., 3., 4., 5., 6., 7., 8., 9.]]),\n",
       " torch.Size([5, 9]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_v_stacked=torch.vstack([x,x,x,x,x])\n",
    "x_v_stacked,x_v_stacked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3819f22",
   "metadata": {},
   "source": [
    "vstack achieves the same result as torch.stack using dim=0 for 1D and higher dimensional tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4801af77",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_stacked\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m x_stacked,x_stacked\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "x_stacked=torch.stack([x,x,x,x,x],dim=2)\n",
    "x_stacked,x_stacked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65db48",
   "metadata": {},
   "source": [
    "Here if we try to do dim=2 it wont work, as this dim should be between 0 and the number of dimensions of concatenated tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad36cfe",
   "metadata": {},
   "source": [
    "### **Squeeze Operation**\n",
    "\n",
    "***torch.squeeze(input: Tensor,dim: optional) -> Tensor***\n",
    "\n",
    "args:\n",
    "\n",
    "- input (Tensor): the input vector\n",
    "\n",
    "- dim (int or tuple of ints,optional): if given the input will be squeezed only in specified dimensions\n",
    "\n",
    "Returns a tensor with all specified dimensions of input of size 1 removed\n",
    "\n",
    "**NOTE:** \n",
    "- The returned tensor shares the storage with the input tensor, so changing contents of one will change contents of another\n",
    "\n",
    "- If tensor has a dimension of size 1 , the squeeze(input) will remove the dimension leading to unexpected errors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5fee7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous tensor:tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]])\n",
      "Previous shape: torch.Size([1, 9])\n",
      "New tensor:tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "New shape: torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "# torch.squeeze()-  removes all single dimensions from a target tensor\n",
    "\n",
    "print(f\"Previous tensor:{x_reshaped}\")\n",
    "print(f\"Previous shape: {x_reshaped.shape}\")\n",
    "\n",
    "x_squeezed=x_reshaped.squeeze()\n",
    "print(F\"New tensor:{x_squeezed}\")\n",
    "print(f\"New shape: {x_reshaped.squeeze().shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd39a9",
   "metadata": {},
   "source": [
    "### **Unsqueeze Operation**\n",
    "\n",
    "***torch.unsqueeze(input,dim) -> Tensor***\n",
    "\n",
    "args:\n",
    "\n",
    "- input(Tensor): the input tensor\n",
    "\n",
    "- dim(int) : the index at which to insert a singleton dimension\n",
    "\n",
    "Returns a new tensor with a dimension of size one inserted at a specific position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dfe9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous target:tensor([5., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "Previous shape:torch.Size([9])\n",
      "New tensor:tensor([[5., 2., 3., 4., 5., 6., 7., 8., 9.]])\n",
      "New shape:torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "# torch.unsqueeze()- adds a single dimension to a target tensor at a specific dim(dimensio)\n",
    "\n",
    "print(f\"Previous target:{x_squeezed}\")\n",
    "print(f\"Previous shape:{x_squeezed.shape}\")\n",
    "\n",
    "x_unsqueezed=x_squeezed.unsqueeze(dim=0)\n",
    "print(f\"New tensor:{x_unsqueezed}\")\n",
    "print(f\"New shape:{x_unsqueezed.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09237f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New tensor:tensor([[5.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]])\n",
      "New shape:torch.Size([9, 1])\n"
     ]
    }
   ],
   "source": [
    "x_unsqueezed=x_squeezed.unsqueeze(dim=1)\n",
    "print(f\"New tensor:{x_unsqueezed}\")\n",
    "print(f\"New shape:{x_unsqueezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525ff78",
   "metadata": {},
   "source": [
    "### **Permute Operation**\n",
    "\n",
    "***torch.permute(input,dims) -> Tensor***\n",
    "\n",
    "args:\n",
    "\n",
    "    input(Tensor): the input tensor\n",
    "    dims(tuple of int): the desired ordering of dimensions\n",
    "\n",
    "Returns a view of the original tensor input with its dimensions permuted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569441ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape:torch.Size([224, 224, 3])\n",
      "New shape:torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# torch.premute- rearranges the dimensions of a tensor in a specified order\n",
    "x_original=torch.rand(size=(224,224,3)) #(height, width, color_channels)\n",
    "\n",
    "x_permuted=x_original.permute(2,0,1) # (colour_channels,height,width) \n",
    "# shifts axis 0->1, 1->2, 2->0\n",
    "\n",
    "print(f\"Previous shape:{x_original.shape}\")\n",
    "print(f\"New shape:{x_permuted.shape}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
